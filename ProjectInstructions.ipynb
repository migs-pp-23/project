{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project guidelines\n",
    "\n",
    "For the final assessment of our course, you must complete a small project in a group.\n",
    "\n",
    "## Choosing topics\n",
    "\n",
    "Please choose at least **2 topics** you're interested in, and email me your choices before **Friday 6th October (evening)**. You can provide a ranking for your preferred topic choices. I will create groups of 3-4 based on topic choices over the weekend.\n",
    "\n",
    "This is a list of proposed project topics:\n",
    "\n",
    "* [A] Categorising data: supervised and unsupervised learning in Python\n",
    "* [B] Finite difference method for the heat equation\n",
    "* [C] Newton and Broyden's methods for nonlinear systems\n",
    "* [D] Polynomial least squares\n",
    "* [E] Loss of significance in floating-point arithmetic\n",
    "* [F] The FEniCS and Dolfin libraries for numerical PDEs\n",
    "* [G] Object-oriented Python for numerical ODEs\n",
    "\n",
    "Each of these topics is introduced in more detail in the following sections. A project outline is given for each topic, including a description of the objectives and guidelines to get you started. The projects are designed to be open-ended -- you can take them in any direction you like. Projects [A], [B], [C], [D] are more guided; projects [E], [F], [G] are more open-ended. I'll be happy to provide further guidance at any point if necessary.\n",
    "\n",
    "For most of these topics, there are multiple possible directions of investigation. The choice of direction is yours -- the important thing is that **you put your Python skills into practice**. Some of these directions may be, for example:\n",
    "\n",
    "* implement an algorithm from scratch, test and investigate different use cases or parameters, try to optimise it as much as possible;\n",
    "* implement two or more algorithms, investigate and compare their behaviour under similar conditions;\n",
    "* use existing libraries (e.g. `scikit-learn` for machine learning, `FEniCS` for PDEs...), and focus on investigating, for instance, efficiency and/or accuracy;\n",
    "* focus on a simple case, and produce interesting/insightful visualisations using Python tools;\n",
    "* etc.\n",
    "\n",
    "Alternatively, you could also propose your **own topic** -- for instance revisit some of your past work, or a result or algorithm from the literature, and try reproducing it in Python. If you do so, then you can still consult the example project briefs below to give you an idea of the scope and expected amount of work.\n",
    "\n",
    "## Support sessions\n",
    "\n",
    "There will be 2 support sessions for the project:\n",
    "\n",
    "- Tuesday 10th October, 11-12, 5.46 Bayes\n",
    "- Tuesday 17th October, 11-12, 5.46 Bayes\n",
    "\n",
    "Please attend these sessions with your group to make progress on your project, and ask for help/advice.\n",
    "\n",
    "## Assessment\n",
    "\n",
    "The projects will be assessed on a pass/fail basis by a **15-min presentation** (+ Q&A), on **Tuesday 24th October, starting at 10am in 5.46 Bayes**.\n",
    "\n",
    "Here is a suggested structure for your presentation:\n",
    "\n",
    "- Introduce your project topic, and state the specific problem/question you have tackled. (2min)\n",
    "- Explain the method you have used to solve the problem; it could be useful to illustrate it with a simple example and/or some useful visualisations produced in Python. (3-5min)\n",
    "- Deep dive: choose a part of your code which you found interesting and/or challenging to implement, or maybe a part which taught you something new about Python, and walk the audience through your code to explain what you did. This doesn't necessarily have to be the \"core\" part of your code, or the part where the main computations happen -- it could also be a utility function you wrote to visualise your results or clean up your data, for instance. (3-5min)\n",
    "- Present your final results and conclude. (2-3min)\n",
    "\n",
    "\n",
    "Here is what I recommend to prepare your presentation:\n",
    "\n",
    "- The RISE extension allows you to (very easily!) create interactive slides from a Jupyter notebook. [Have a look at the documentation](https://rise.readthedocs.io/en/stable/). You'll need to install the extension on your computer with `conda install -c conda-forge rise`.\n",
    "- Write the bulk of your code as functions in one or more separate modules (.py scripts) in the same folder as your notebook.\n",
    "- Import your module(s) into the notebook to use your functions.\n",
    "\n",
    "The idea is to create a notebook for your slides, and only have relatively short code snippets in the slides, that you can run live during your presentation. (If your code takes quite a long time to run, then you can run it in advance and show the results statically instead.)\n",
    "\n",
    "For the \"deep dive\" part, you can exit the slides and show the code directly inside a .py module, or you can copy it over into a code cell directly in your slides -- whatever is most convenient.\n",
    "\n",
    "There is an example in the \"Example\" folder. In Jupyter Notebook, click \"View\" > \"Cell toolbar\" > \"Slideshow\" to display a bar at the top of each cell allowing you to structure and order your slides. Launch the notebook, and click the \"Enter/Exit RISE slideshow\" button in the toolbar at the top (the icon looks like a bar chart). In Slideshow mode, you can still edit and run the code in the code cells.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project A: Categorising data: supervised and unsupervised learning in Python\n",
    "\n",
    "In this project, you will investigate one or both of the following machine learning problems:\n",
    "\n",
    "- Supervised **classification**, where all items in a data set are classified into two or more labelled categories. The number of categories and their labels are known in advance.\n",
    "- Unsupervised **clustering**, where the data set is partitioned into a number of *clusters*, based on the similarity between items. The number of categories or labels are *not necessarily* known in advance.\n",
    "\n",
    "The aim of the project is to perform a classification or clustering task, using at least one method, on at least one dataset.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "A good reference for machine learning methods is the book [*Elements of Statistical Learning*, by Hastie, Tibshirani, and Friedman](https://web.stanford.edu/~hastie/ElemStatLearn/). You should be able to find all the mathematical background needed for a wide variety of statistical learning methods, and plenty of ideas for evaluation and further investigation.\n",
    "\n",
    "The [scikit-learn module](https://scikit-learn.org/stable/) provides a number of algorithms for classification and clustering problems, as well as tools for pre-processing data. You may wish to use these for your project, for instance if you want to investigate the performance of different algorithms with a given dataset, or to evaluate the usefulness of different pre-processing methods. Start with one of the [tutorial examples](https://scikit-learn.org/stable/tutorial/basic/tutorial.html) to get to grips with the module's functionality.\n",
    "\n",
    "Alternatively, you may wish to implement an algorithm from first principles to solve either of these problems. In this case, I would probably recommend a classification algorithm which is relatively straightforward to implement: the *$k$-nearest-neighbours* algorithm.\n",
    "\n",
    "#### $k$-nearest-neighbours\n",
    "\n",
    "The $k$-nearest-neighbours algorithm is a classic machine learning algorithm used for classification problems. The data is assumed to be separated into a *training dataset*, where the class (the label) of each item is known, and a *test dataset*, which contains the unlabelled data that you would like to classify.\n",
    "\n",
    "A basic implementation is as follows: for each item in the test dataset, find the $k$ nearest items in the training dataset -- these are your $k$ nearest neighbours. The class of your unlabelled item is then determined by majority voting amongst the classes of these $k$ neighbours. In the case where two or more classes are tied in the vote, the tie is resolved by taking the class of the nearest data item in the training dataset.\n",
    "\n",
    "There are 3 different aspects of this algorithm which you have control over:\n",
    "\n",
    "* $k$, the number of nearest neighbours,\n",
    "* the *distance metric*, i.e. what is meant by \"nearest\" -- you could start with computing the Euclidean distance between items, and then experiment with other metrics,\n",
    "* the voting method -- majority voting is not the only possibility; for instance, you could weigh the votes according to distances, so that the closest of the $k$ neighbours have more influence on the classification of the test item.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "[This archive](https://archive.ics.uci.edu/ml/datasets.php) provides hundreds of different data sets which you can use for your project. Different datasets are suited for different machine learning problems -- you will be able to find many different appropriate datasets for both classification and clustering. If you wish to implement an algorithm from first principles, you may wish to use a fairly simple dataset -- a couple of suggestions:\n",
    "\n",
    "* [The iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris) is a small but very well-known dataset, containing 150 labelled data points, each with 4 attributes representing 4 different measurements on iris flowers. Each data point belongs to one of three species of iris flower.\n",
    "* [The MNIST dataset](http://yann.lecun.com/exdb/mnist/) is much larger, and also very well-known. Each data point is a $28\\times 28$ pixel greyscale image of a handwritten digit (0-9), each with 784 attributes (the intensity of each pixel). Each data point (in the training set) is labelled with the correct number.\n",
    "* [The wine dataset](http://archive.ics.uci.edu/ml/datasets/Wine) contains the results of chemical analysis of 178 different wines, made with grapes from one of three possible types of vine.\n",
    "* scikit-learn comes with a few [built-in datasets](https://scikit-learn.org/stable/datasets/index.html#toy-datasets), including iris and wine.\n",
    "* Alternatively, particularly for clustering problems, you may wish to generate your own synthetic data -- again, you can do this [with scikit-learn](https://scikit-learn.org/stable/datasets/index.html#generated-datasets).\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) - a great free online book.\n",
    "* [*Elements of Statistical Learning*, by Hastie, Tibshirani, and Friedman](https://web.stanford.edu/~hastie/ElemStatLearn/) -- a solid textbook reference.\n",
    "* [Start Here With Machine Learning](https://machinelearningmastery.com/start-here/) -- a series of guides and tutorials at different levels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project B: Finite difference method for the heat equation\n",
    "\n",
    "In this project, you will produce simulations of the evolution of the temperature distribution in a medium, by computing numerical solutions to the heat equation using the **finite difference method**, under different initial and boundary conditions.\n",
    "\n",
    "### The 1D heat equation\n",
    "\n",
    "Let the temperature distribution over a thin rod of length $L$ be defined as $u(x, t)$, where $t\\geq 0$ and $x \\in [0, L]$. The evolution of this temperature distribution is governed by the *1D heat equation*,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2},\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the thermal diffusivity of the medium. The full initial-boundary value problem is defined by this equation, plus an initial condition $u(x, 0) = f(x)$, representing the temperature distribution along the rod at time $t=0$, and two boundary conditions, one at each end of the rod. For example, the temperature may be fixed at 0 at both ends, i.e.\n",
    "\n",
    "$$\n",
    "u(0, t) = u(L, t) = 0.\n",
    "$$\n",
    "\n",
    "### Discretisation\n",
    "\n",
    "The idea behind finite difference methods is to compute approximated solutions to an initial-boundary value problem like this one, by discretising the domain of definition of $u$ and approximating partial derivative operators with difference operators.\n",
    "\n",
    "Let $\\Delta t >0$ and $\\Delta x >0$ denote temporal and spatial step sizes, respectively. Consider now a discrete function $u_l^n$, defined over $n \\in \\{0, 1, 2, \\dots\\}$ and $l \\in \\{0, \\dots, N\\}$ (where $N=\\frac{L}{\\Delta x}$), such that $u_l^n$ is an approximation of the temperature distribution $u$ at time $t_0=n\\Delta t$ and $x_0 = l\\Delta x$, that is\n",
    "$$\n",
    "u_l^n \\approx u(l\\Delta x, n\\Delta t) = u(x_0, t_0).\n",
    "$$\n",
    "\n",
    "We therefore have, for example,\n",
    "\n",
    "\\begin{align}\n",
    "u_{l+1}^n &\\approx u((l+1)\\Delta x, n\\Delta t) = u(x_0 + \\Delta x, t_0), \\\\\n",
    "u_l^{n-1} &\\approx u(l\\Delta x, (n-1)\\Delta t) = u(x_0, t_0 - \\Delta t).\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Partial derivatives may be approximated by finite differences. To see this, take the definition of the derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t}(x, t_0) = \\lim_{\\Delta t \\to 0} \\frac{u(x, t_0 + \\Delta t) - u(x, t_0)}{\\Delta t}.\n",
    "$$\n",
    "\n",
    "A finite difference approximation of $\\frac{\\partial u}{\\partial t}$ can be defined by letting $\\Delta t$ take a small, finite value:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t}(x, t_0) \\approx \\frac{u(x, t_0 + \\Delta t) - u(x, t_0)}{\\Delta t}\n",
    "\\approx \\frac{u_l^{n+1} - u_l^n}{\\Delta t}.\n",
    "$$\n",
    "\n",
    "This is the *forward* temporal difference, since it requires an approximation of $u(x, t)$ at a point *forward* in time, $t_0 + \\Delta t$.\n",
    "\n",
    "Similarly, a *centred* finite difference approximation of $\\frac{\\partial^2 u}{\\partial x^2}$ is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial x^2} \\approx \\frac{u_{l-1}^n - 2u_l^n + u_{l+1}^n}{(\\Delta x)^2}.\n",
    "$$\n",
    "\n",
    "Using these approximations, we can write a finite difference *scheme* to approximate the 1D heat equation -- the *forward-time, centred-space (FTCS) scheme*:\n",
    "\n",
    "$$\n",
    "\\frac{u_l^{n+1} - u_l^n}{\\Delta t} = \\alpha \\frac{u_{l-1}^n - 2u_l^n + u_{l+1}^n}{(\\Delta x)^2}.\n",
    "$$\n",
    "\n",
    "### Computing numerical solutions\n",
    "\n",
    "We can use our FTCS finite difference scheme to compute numerical solutions to the heat equation. The idea is to rearrange it into a *recursion* in time, so that the temperature distribution at a given time step can be directly computed from the distribution at previous time steps. The FTCS scheme can be rearranged to give $u_l^{n+1}$ in terms of $u_{l-1}^n$, $u_l^n$, and $u_{l+1}^n$, all known from the previous step:\n",
    "\n",
    "$$\n",
    "u_l^{n+1} = \\frac{\\alpha \\Delta t}{(\\Delta x)^2} \\left(u_{l+1}^n + u_{l-1}^n\\right)\n",
    "+ \\left(1 - \\frac{2\\alpha \\Delta t}{(\\Delta x)^2}\\right) u_l^n.\n",
    "$$\n",
    "\n",
    "The initial condition gives the temperature distribution at initial time -- that is, we can set $u_l^0 = u(l\\Delta x, 0)$. The temperature distribution at future time steps can now be computed recursively, each time for all points $l$ of the rod.\n",
    "\n",
    "**Compute** the approximate solution to the 1D heat equation with the FTCS scheme, using the following parameters:\n",
    "\n",
    "* $\\alpha = 0.1$\n",
    "* $L=1$\n",
    "* $u_0^n = u_N^n = 0$ (fixed boundary conditions)\n",
    "* $\\Delta t = 10^{-4}$\n",
    "* $N = 100$\n",
    "\n",
    "Initialise the temperature distribution with random values between -1 and 1, and run the simulation for 1000 time steps. Plot the solution dynamically, at every iteration (or every $m$th iteration, with your choice of $m$) in the loop, to visualise how the temperature distribution changes over time along the rod -- [this](https://stackoverflow.com/a/39853938) may be helpful.\n",
    "\n",
    "\n",
    "### Further investigation\n",
    "\n",
    "Here are some ideas for further investigation -- you absolutely don't need to try all of them, and you could also try something I haven't mentioned here.\n",
    "\n",
    "#### Different parameters\n",
    "\n",
    "Try different initial distributions, and different sets of boundary conditions. You could have, for instance, zero temperature at one end, and a fixed, positive temperature at the other -- find out what the temperature distribution settles to. Try starting with a random temperature distribution, or a distribution with one or more peaks, where the rod would have been pre-heated at specific locations. Try different materials, with different values of $\\alpha$.\n",
    "\n",
    "You can also include point heat sources, distributed sources, and even moving sources -- in this case, you could use linear interpolation to \"spread\" the source between two grid points, depending on the position $x_s(n\\Delta t)$ of the source at time step $n$. Some of the resources listed below detail how to include source terms.\n",
    "\n",
    "#### Stability condition\n",
    "\n",
    "A finite difference scheme is **stable** if it has no solutions which grow exponentially over time. This is to say that *all* discrete solutions of the form $u_l^n = e^{n\\lambda \\Delta t}e^{i\\beta \\Delta x}$ must satisfy $\\mathrm{Re}(\\lambda)\\leq 0$ ($\\lambda \\in \\mathbb{C}, \\beta \\in \\mathbb{R}, i = \\sqrt{-1}$). Substituting this test solution into the scheme and enforcing the condition on $\\lambda$ will lead to an inequality relating $\\alpha$, $\\Delta t$, and $\\Delta x$ -- this is the **stability condition**. Find it, and verify it by running simulations with parameter values which violate it -- the solution should explode.\n",
    "\n",
    "#### Other schemes\n",
    "\n",
    "Other finite difference operators may be derived to approximate $\\frac{\\partial u}{\\partial t}$ at different orders of accuracy, by using Taylor series expansions of $u$ around $t_0$ truncated at different orders. For example, the *backwards* temporal difference is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t}(x, t_0) \\approx \\frac{u(x, t_0) - u(x, t_0 - \\Delta t)}{\\Delta t}\n",
    "\\approx \\frac{u_l^n - u_l^{n-1}}{\\Delta t}.\n",
    "$$\n",
    "\n",
    "The backward-time, centred-space (BTCS) scheme is therefore given by:\n",
    "\n",
    "$$\n",
    "\\frac{u_l^n - u_l^{n-1}}{\\Delta t} = \\frac{u_{l-1}^n - 2u_l^n + u_{l+1}^n}{(\\Delta x)^2}.\n",
    "$$\n",
    "\n",
    "Another scheme, called the *Crank-Nicolson (CN) scheme*, relies on discretising the temporal derivative using the trapezoid rule, and is given by\n",
    "\n",
    "$$\n",
    "\\frac{u_l^{n+1} - u_l^n}{\\Delta t} = \\frac{1}{2}\\left(\n",
    "\\frac{u_{l-1}^{n+1} - 2u_l^{n+1} + u_{l+1}^{n+1}}{(\\Delta x)^2} +\n",
    "\\frac{u_{l-1}^n - 2u_l^n + u_{l+1}^n}{(\\Delta x)^2}\\right).\n",
    "$$\n",
    "\n",
    "These two schemes are *unconditionally stable* (you could try to prove this). However, they are *implicit*: if you try to rearrange them into a temporal recursion, you will see that the unknowns to solve for at each time step depend on each other. The way to compute the temperature distribution at the next time step is therefore to write the recursion as a linear system with $N$ equations and $N$ unknowns, and solve it using, for example, `np.linalg.solve()`. The unknowns for the BTCS scheme are the $u_l^n, l \\in \\{0, \\dots, N\\}$, and the unknowns for the CN scheme are the $u_l^{n+1}, l \\in \\{0, \\dots, N\\}$.\n",
    "\n",
    "#### Other PDEs\n",
    "\n",
    "It should be *relatively* straightforward to extend this to the 2D heat equation, to simulate the evolution of the temperature distribution over, say, a rectangular plate. You could also try to compute solutions to the wave equation, for instance, in 1D or 2D.\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [*Finite Difference Schemes and Partial Differential Equations*, J. Strikwerda](https://doi.org/10.1137/1.9780898717938) -- a great textbook, available online through the library. In particular, Section 6.3 presents a number of finite difference schemes for the 1D heat equation.\n",
    "* [Solving the Heat, Laplace and Wave equations using finite difference methods](https://www.math.ubc.ca/~peirce/M257_316_2012_Lecture_8.pdf) -- lecture notes by Prof. Anthony Peirce, University of British Columbia\n",
    "* [Thermal diffusivity of different materials](https://en.wikipedia.org/wiki/Thermal_diffusivity)\n",
    "* [The diffusion equation](https://warwick.ac.uk/fac/cross_fac/complexity/study/msc_and_phd/co906/co906online/lecturenotes_2009/chap3.pdf) -- lecture notes by Dr Colm Connaughton at Warwick University\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project C: Newton and Broyden's methods for nonlinear systems\n",
    "\n",
    "Consider the following system of $n$ nonlinear equations in $n$ independent variables\n",
    "\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}) = \\mathbf{0},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}\\in \\mathbb{R}^n$, and $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^n$ is a nonlinear vector-valued function. Suppose that $\\mathbf{f}$ is differentiable, and that this system has a unique solution $\\mathbf{x}^\\star \\in \\mathbb{R}^n$.\n",
    "\n",
    "In this project, you will implement two *iterative* methods to compute an approximate solution to a nonlinear system: **Newton's method** and **Broyden's method**. For both these methods, the idea is to start with an initial guess $\\mathbf{x}^{(0)}$, and iteratively refine this guess, until convergence to the solution is achieved.\n",
    "\n",
    "Both methods will be used to compute the solution to the following system:\n",
    "\n",
    "\\begin{align}\n",
    "    f_1(\\mathbf{x}) &= 2x_1 - x_2 + \\frac{a^2}{2} (x_1 + a + 1)^3, \\nonumber \\\\\n",
    "    f_i(\\mathbf{x}) &= 2x_i - x_{i-1} - x_{i+1} + \\frac{a^2}{2} (x_i + ia + 1)^3, \\qquad \\qquad i=2,\\ldots,n-1, \\nonumber \\\\\n",
    "    f_n(\\mathbf{x}) &= 2x_n - x_{n-1} + \\frac{a^2}{2} (x_n + na + 1)^3,\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $a = \\frac{1}{n+1}$.\n",
    "\n",
    "### Newton's method\n",
    "\n",
    "The next guess $\\mathbf{x}^{(k+1)}$ in Newton's method is computed as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\big(\\mathbf{J}^{(k)}\\big)^{-1} \\mathbf{f}(\\mathbf{x}^{(k)}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{J}^{(k)}$ is the Jacobian matrix of $\\mathbf{f}(\\mathbf{x}^{(k)})$. At each iteration, Newton's method therefore requires evaluating the function $\\mathbf{f}(\\mathbf{x}^{(k)})$, the Jacobian matrix $\\mathbf{J}^{(k)}$, and solving a linear system.\n",
    "\n",
    "**Compute** the solution of the test system given above using Newton's method, with the initial guess\n",
    "\n",
    "$$\n",
    "x_i^{(0)} = ia(ia - 1), \\qquad i=1,\\ldots,n,\n",
    "$$\n",
    "\n",
    "for different values of $n$ (up to $n \\sim 2000$). Use functionality provided by [the `time` module](https://docs.python.org/3/library/time.html#time.time) to find out the most computationally expensive operations in each iteration, and report your findings.\n",
    "\n",
    "### Broyden's method\n",
    "\n",
    "A class of algorithms, called *quasi-Newton methods*, take Newton's method as a starting point. In order to reduce computational cost, these methods approximate the Jacobian matrix instead of evaluating it at every iteration. They are particularly useful when the Jacobian is computationally expensive to evaluate.\n",
    "\n",
    "The Broyden step substitutes the Jacobian matrix in the Newton step with an approximation $\\tilde{\\mathbf{J}}^{(k)}$, computed iteratively from $\\tilde{\\mathbf{J}}^{(k-1)}$:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{J}}^{(k)} =\n",
    "\\tilde{\\mathbf{J}}^{(k-1)} +\n",
    "\\left( \\mathbf{y}^{(k)} - \\tilde{\\mathbf{J}}^{(k-1)} \\mathbf{h}^{(k)} \\right)\n",
    "\\frac{{\\mathbf{h}^{(k)}}^{\\mathsf{T}}}{\\|\\mathbf{h}^{(k)}\\|_2^2},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{y}^{(k)} = \\mathbf{f}(\\mathbf{x}^{(k)}) - \\mathbf{f}(\\mathbf{x}^{(k-1)}), \\qquad\n",
    "\\mathbf{h}^{(k)} = \\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}.\n",
    "$$\n",
    "\n",
    "An important thing to note here is that $\\tilde{\\mathbf{J}}^{(k)}$ is a **rank-1 update** of $\\tilde{\\mathbf{J}}^{(k-1)}$, and therefore its *inverse* can also be updated iteratively, using the *Sherman-Morrison formula* -- see the Resources linked below.\n",
    "\n",
    "**Compute** the solution of the test system using Broyden's method and the Sherman-Morrison formula. You will first need to initialise $(\\tilde{\\mathbf{J}}^{(0)})^{-1} = (\\mathbf{J}^{(0)})^{-1}$ explicitly.\n",
    "\n",
    "Use different values of $n$, up to $n \\sim 2000$, and measure computation times as before.\n",
    "\n",
    "### Further investigation\n",
    "\n",
    "Try both methods on different nonlinear systems, perhaps systems you have previously seen in your studies or in your research.\n",
    "\n",
    "Investigate and report the convergence properties of both methods, using different systems and different initial guesses.\n",
    "\n",
    "There is plenty of recent literature on modified and improved Newton and Broyden methods -- you could try implementing one of these.\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [Solving nonlinear equations with Newton's method, C.T. Kelley](https://epubs.siam.org/doi/abs/10.1137/1.9780898718898.ch4) -- Chapter 4: Broyden's method\n",
    "* [Sherman-Morrison-Woodbury](https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec12.pdf) -- lecture notes by David Bindel, Cornell University\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project D: Polynomial least squares\n",
    "\n",
    "Suppose we have a set of data points $\\{x^{(i)},y^{(i)}\\}_{i=1,\\ldots,m}$. The goal is to fit a a polynomial of degree $(n-1)$ to the data, with coefficients $b_j, j=0,\\ldots,n-1$, of the form\n",
    "\n",
    "$$\n",
    "y = b_0 + x b_1 + x^2 b_2 + \\ldots + x^{n-1} b_{n-1}\n",
    "= \\sum_{j=0}^{n-1} x^j b_j.\n",
    "$$\n",
    "\n",
    "When $m > n$, the polynomial coefficients may be estimated so that the sum of squared residuals between the LHS and the RHS is minimised over the set of data points. The polynomial may be written in matrix-vector form, for all data points, as an overdetermined system:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{Xb},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}$ is the Vandermonde matrix $\\mathbf{X}\\in \\mathbb{R}^{m\\times n}$, with elements $\\mathbf{X}_{ij} = (x^{(i)})^{j-1}$ for $j=1,\\ldots,n$. This system has no exact solution -- however, there exists one set of coefficients $\\mathbf{b} \\in \\mathbb{R}^n$ which minimises $\\|\\mathbf{Xb} - \\mathbf{y}\\|_2^2$, the sum of squared residuals. It can be shown that this minimiser solves the **normal equations**\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T\\mathbf{Xb} = \\mathbf{X}^T \\mathbf{y}\n",
    "\\qquad \\Rightarrow \\qquad\n",
    "\\mathbf{b} = \\mathbf{X}^{\\dagger}\\mathbf{y}, \\quad \\text{where }\n",
    "\\mathbf{X}^{\\dagger} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T.\n",
    "$$\n",
    "\n",
    "The matrix $\\mathbf{X}^{\\dagger} \\in \\mathbb{R}^{n\\times n}$ is the *Moore-Penrose pseudo-inverse* of $\\mathbf{X}$.\n",
    "\n",
    "You can generate artificial data using a similar process as that seen in W2.4 to generate the noisy sinusoid -- use some polynomial as an underlying function instead, and try to estimate the coefficients from the noisy data by solving the normal equations with, e.g., `np.linalg.solve()`.\n",
    "\n",
    "Try polynomials of different degrees, with coefficients of different magnitudes, with more or less noisy data.\n",
    "\n",
    "If you recall one of the past worksheet problems, you may have noticed that the matrix $\\mathbf{X}^T\\mathbf{X}$ is very badly conditioned, which is likely to introduce very large numerical errors when solving the normal equations. There is a way to avoid having to form and invert this matrix: compute and use the **singular value decomposition** (SVD) of $\\mathbf{X}$ (see lecture notes linked in Resources).\n",
    "\n",
    "Finally, a common issue with polynomial regression is **overfitting** -- this occurs when a model is complex enough to correspond very closely to the particular set of data points you are training it with, but does not generalise well, and will not be able to predict future observations. Instead of capturing the relationship between input and output in a dataset, an overfitted model describes the statistical noise in the data.\n",
    "\n",
    "A method to avoid overfitting is **regularisation** -- here, we will look at a special case of Tikhonov regularisation (a.k.a. ridge regression for the statisticians in the audience). The regularised normal equations are given by\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^T\\mathbf{X} + \\mu \\mathbf{I})\\mathbf{b} = \\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "\n",
    "where $\\mu>0$ is a regularisation parameter. The idea is to introduce a penalty term, so that the polynomial coefficients $\\mathbf{b}$ (particularly the high-order coefficients) remain relatively small.\n",
    "\n",
    "Rewriting the regularised normal equations using the SVD of $\\mathbf{X}$ leads to a simple, explicitly computed solution $\\mathbf{b}$. Try implementing this with different values of $\\mu$, and try to find the best value for a given problem, large enough that overfitting is avoided, but small enough that the model isn't underfitted.\n",
    "\n",
    "### Further investigation\n",
    "\n",
    "This is straightforwardly generalisable to the multivariate case, when the data points have more than one attribute $x$. Try to apply the same method to perform polynomial regression on some of the datasets linked in the resources (also see the Project A description).\n",
    "\n",
    "Versions of polynomial regression are implemented in Numpy (`np.polyfit`), scikit-learn, and many other modules. Compare your results with some of these existing implementations.\n",
    "\n",
    "Polynomial regression is still a type of multiple *linear* regression -- the model is a linear function of the unknown polynomial coefficients. More elaborate regressions can be performed by using other basis functions than polynomials of $x$ -- you can read about these in the linked resources, and try to implement some of them to perform regression on different datasets.\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [*Elements of Statistical Learning*, by Hastie, Tibshirani, and Friedman](https://web.stanford.edu/~hastie/ElemStatLearn/) -- Chapter 5: Basis expansions and regularization\n",
    "* [Least squares, pseudo-inverse, and SVD](http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf) -- lecture notes by Guido Gerig, New York University\n",
    "* [Polynomial regression: extending linear models with basis functions](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions) -- scikit-learn documentation\n",
    "* [Dataset archive](https://archive.ics.uci.edu/ml/datasets.php)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project E: Loss of significance in floating-point arithmetic\n",
    "\n",
    "In this project, you will explore and demonstrate different issues related to *loss of significance* when using floating-point numbers. You will investigate at least two example problems where numerical error plays a significant role; when alternative solutions exist to minimise these issues, you will demonstrate these.\n",
    "\n",
    "A few example problems to investigate could be (but are not limited to):\n",
    "\n",
    "* The quadratic formula (a classic!)\n",
    "* Heron's formula for the area of needle triangles\n",
    "* Small step sizes in finite difference approximations\n",
    "* Solving the linear system $Ax=b$ when $A$ is ill-conditioned\n",
    "* Propagation of error in large sums\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) -- I have linked this resource in one of the worksheets. You should be able to find plenty of information there.\n",
    "* [Loss of Significance - The loss of numerical accuracy in computer calculation](https://zipcon.net/~swhite/docs/math/loss_of_significance.html)\n",
    "* [Miscalculating Area and Angles of a Needle-like Triangle, W. Kahan](https://people.eecs.berkeley.edu/~wkahan/Triangle.pdf)\n",
    "* [Ill-conditioned systems](http://engrwww.usask.ca/classes/EE/840/notes/ILL_Conditioned%20Systems.pdf)\n",
    "* [Ill-conditioned matrices](http://www.cs.uleth.ca/~holzmann/notes/illconditioned.pdf)\n",
    "* [How and How Not to Sum Floating-Point Numbers](http://www.phys.uconn.edu/~rozman/Courses/P2200_11F/downloads/sum-howto.pdf)\n",
    "* [Error and computer arithmetic](http://www.math.pitt.edu/~trenchea/math1070/MATH1070_2_Error_and_Computer_Arithmetic.pdf)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project F: The FEniCS and Dolfin libraries for numerical PDEs\n",
    "\n",
    "Dolfin is a Python-based package for advanced scientific computing.  Documentation is available [here](https://fenicsproject.org/olddocs/dolfin/latest/python/demos.html), along with a collection of excellent demos that illustrate the use of the package for various problems.  \n",
    "\n",
    "In this project, you will use FEniCS to compute and visualise solutions for a PDE of your choice. Here is a suggested example:\n",
    "\n",
    "* Install FEniCS on your computer and study the FEniCS tutorial at [https://fenicsproject.org/tutorial/](https://fenicsproject.org/tutorial/).\n",
    "* Explore the **Cahn-Hilliard equation** demo by running the code and graphing solutions.\n",
    "* Modify the Cahn-Hilliard solver above to implement one of the variant methods described in [Ref5].\n",
    "\n",
    "\n",
    "### Resources\n",
    "\n",
    "* *Introduction to Scientific Programming in Python*, freely available [here](https://library.oapen.org/bitstream/id/56d27e73-e92a-4398-8198-239be7aacc93/2020_Book_IntroductionToScientificProgra.pdf).\n",
    "\n",
    "* *Numerical Methods*, Burden and Faires, 3 copies in the library (but not online?).\n",
    "\n",
    "*  [Slides from a course at BU](https://www.bu.edu/tech/files/2020/02/Numerical-and-Scientific-Computing-in-Python-v0.1.2.pdf) on scientific computing in Python.\n",
    "\n",
    "* FEniCS documentation available [here](https://fenicsproject.org/olddocs/dolfin/latest/python/demos.html).\n",
    "\n",
    "* *Numerical methods for solving the Cahn-Hilliard equation and its applicability to related Energy-based models*, T. Tierra and  F.Guillen-Gonzalez, 2013.  [https://core.ac.uk/download/pdf/51405273.pdf](https://core.ac.uk/download/pdf/51405273.pdf)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project G: Object-oriented Python for numerical ODEs\n",
    "\n",
    "For this project, study the use of **classes** in Python. There is a good discussion of this in Chapters 8 and 9 of [*Introduction to Scientific Programming in Python*](https://library.oapen.org/bitstream/id/56d27e73-e92a-4398-8198-239be7aacc93/2020_Book_IntroductionToScientificProgra.pdf). In Chapter 9 of that book there are some examples for numerical differentiation and integration.\n",
    "\n",
    "Define a class-based framework for solving ordinary differential equations using numerical methods like Euler's method or a 4th order Runge-Kutta method.\n",
    "\n",
    "### Resources\n",
    "\n",
    "* *Introduction to Scientific Programming in Python*, freely available [here](https://library.oapen.org/bitstream/id/56d27e73-e92a-4398-8198-239be7aacc93/2020_Book_IntroductionToScientificProgra.pdf).\n",
    "\n",
    "* *Numerical Methods*, Burden and Faires, 3 copies in the library (but not online?).\n",
    "\n",
    "* [Classes - Python tutorial](https://docs.python.org/3.9/tutorial/classes.html)\n",
    "\n",
    "* *Python Programming and Numerical Methods - A Guide for Engineers and Scientists* - Chap. 7: [Object-oriented programming](https://pythonnumericalmethods.berkeley.edu/notebooks/chapter07.00-Object-Oriented-Programming.html)\n",
    "\n",
    "* *Python Programming and Numerical Methods - A Guide for Engineers and Scientists* - Chap. 22: [Ordinary Differential Equation - Initial Value Problems](https://pythonnumericalmethods.berkeley.edu/notebooks/chapter22.00-ODE-Initial-Value-Problems.html)\n",
    "\n",
    "* *Applied Scientific Computing* - [Differential equations](https://link-springer-com.ezproxy.is.ed.ac.uk/chapter/10.1007/978-3-319-89575-8_7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
